{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffe19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beec916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torchvision import models\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import torch.nn.functional as F\n",
    "#from einops import rearrange, repeat\n",
    "#from einops.layers.torch import Rearrange\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from torch import einsum\n",
    "import cv2\n",
    "import scipy.misc\n",
    "#import utils\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#file_path = \"../simulate_data/squat_data.csv\"\n",
    "file_path = \"sim_squat_data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3181dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['vel_x', 'vel_y', 'vel_z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df['updown'].shift(1).fillna(0)\n",
    "squat_ends = df[(tmp == 1) & (df['updown'] == 0)].index\n",
    "print(f\"Total squat ends: {len(squat_ends)}\")\n",
    "\n",
    "squat_sets = []\n",
    "\n",
    "start_index = 0\n",
    "for end_index in squat_ends:\n",
    "    one_set = df.iloc[start_index:end_index]\n",
    "\n",
    "    squat_sets.append(one_set)\n",
    "\n",
    "    #squat_sets.append(one_set)\n",
    "    start_index = end_index + 1\n",
    "\n",
    "print(f\"Total sets: {len(squat_sets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 sample당 15frames (현재 포함 과거 10개, 미래 5개), stride 5\n",
    "X = []\n",
    "y = []\n",
    "for one_set in squat_sets:\n",
    "    for i in range(9, len(one_set) - 15, 5):\n",
    "        X.append(one_set.iloc[i-9:i+6, 1:].values)\n",
    "        y.append(one_set.iloc[i]['updown'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "full_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X, dtype=torch.float32),\n",
    "    torch.tensor(y, dtype=torch.long)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97559671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"index_dict.json\", \"r\") as f:\n",
    "    index_dict = json.load(f)\n",
    "\n",
    "train_idx = index_dict[\"train_idx\"]\n",
    "val_idx = index_dict[\"val_idx\"]\n",
    "test_idx = index_dict[\"test_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68289250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e34b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e17392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biGRU(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length, laten_size):\n",
    "        super(biGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                         num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "        self.fc_latent = nn.Linear(hidden_size*2, laten_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        #h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h_0)\n",
    "        content_out = self.fc_latent(out[:, 10, :])\n",
    "        dis_out = self.fc(out[:, 10, :]) # 과거 10개, 현재 5개\n",
    "        #out = self.fc(out[:, -1, :]) # 과거 15개개\n",
    "\n",
    "        return content_out, dis_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_size = 7\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "seq_length = 15\n",
    "latent_size = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f317aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "def initialize_gru_weights(gru):\n",
    "    for name, param in gru.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            init.xavier_uniform_(param.data)\n",
    "        elif 'bias' in name:\n",
    "            init.constant_(param.data, 0)\n",
    "\n",
    "# 초기화 실행\n",
    "set_seed(42)\n",
    "model = biGRU(num_classes, input_size, hidden_size, num_layers, seq_length, latent_size).to(device)\n",
    "model.apply(initialize_weights)\n",
    "initialize_gru_weights(model.gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0be19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_size = 7\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "seq_length = 15\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        #_, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device, phase=\"validation\"):\n",
    "    global best_val_loss\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=f\"Evaluating {phase}\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            _, outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            labels = labels.unsqueeze(1).float()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f}, {phase.capitalize()} Accuracy: {accuracy:.2f}%\")\n",
    "    if phase == \"validation\":\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "        if epoch_loss < best_val_loss:\n",
    "            best_val_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), \"sim_best_model.pth\")\n",
    "            print(\"Best model saved!\")\n",
    "    if phase == \"test\":\n",
    "        test_accuracy = accuracy\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "        f1_score_result = f1_score(all_labels, all_predictions)\n",
    "        recall_score_result = recall_score(all_labels, all_predictions)\n",
    "        precision_score_result = precision_score(all_labels, all_predictions)\n",
    "\n",
    "        return f1_score_result, recall_score_result, precision_score_result, all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ca220",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    evaluate(model, val_loader, criterion, device, phase=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacba740",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"sim_best_model.pth\"))\n",
    "model = model.to(device)\n",
    "f1_score_result, recall_score_result, precision_score_result, all_labels, all_predictions = evaluate(model, test_loader, criterion, device, phase=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 Score: {f1_score_result:.4f}\")\n",
    "print(f\"Recall Score: {recall_score_result:.4f}\")\n",
    "print(f\"Precision Score: {precision_score_result:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss}\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8772c30",
   "metadata": {},
   "source": [
    "Style encoder & decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class style_encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, latent_size):\n",
    "        super(style_encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_size * 2, latent_size)\n",
    "        self.var = nn.Linear(hidden_size * 2, latent_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(x, (h_0, c_0))\n",
    "        style_frame = out[:, 10, :]\n",
    "\n",
    "        mu = self.fc_mu(style_frame)\n",
    "        log_var = self.var(style_frame)\n",
    "\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e07083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_size, hidden_size)\n",
    "        self.latent_to_input = nn.Linear(latent_size, input_size)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        self.seq_len = 15  # Length of the sequence to generate\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.latent_to_hidden(z)\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        x = self.latent_to_input(z)\n",
    "\n",
    "        outputs = []\n",
    "        for _ in range(self.seq_len):\n",
    "            h, c = self.lstm_cell(x, (h, c))\n",
    "            out = self.output_layer(h)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "            x = out  # Use the output as the next input\n",
    "\n",
    "        recon_seq = torch.cat(outputs, dim=1)\n",
    "        return recon_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, style_encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.style_encoder = style_encoder\n",
    "        #self.content_encoder = content_encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.tensor([0.0, 0.0]))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x, z_c):\n",
    "        #z_c, pred = self.content_encoder(x)\n",
    "        mu, logvar = self.style_encoder(x)\n",
    "        z_s = self.reparameterize(mu, logvar)\n",
    "\n",
    "        weights = F.softmax(self.alpha, dim=0)\n",
    "        alpha_c, alpha_s = weights\n",
    "\n",
    "        z_c = alpha_c * z_c\n",
    "        z_s = alpha_s * z_s\n",
    "\n",
    "        z = torch.cat((z_c, z_s), dim=1)  # Concatenate content and style latent vectors\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MineNetwork(nn.Module):\n",
    "    def __init__(self, content_dim, style_dim, hidden_dim=128):\n",
    "        super(MineNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(content_dim + style_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_c, z_s):\n",
    "        # z_c, z_s shape: [batch_size, dim]\n",
    "        joint_input = torch.cat([z_c, z_s], dim=1)\n",
    "        return self.model(joint_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_loss(mine_net, z_c, z_s):\n",
    "\n",
    "    # joint distribution (same sample pairs)\n",
    "    joint_scores = mine_net(z_c, z_s)\n",
    "\n",
    "    # marginal distribution (shuffle z_s to break correlation)\n",
    "    z_s_shuffled = z_s[torch.randperm(z_s.size(0))]\n",
    "    marginal_scores = mine_net(z_c, z_s_shuffled)\n",
    "\n",
    "    # MINE loss = -(E[joint] - log(E[exp(marginal)]))\n",
    "    loss = -(torch.mean(joint_scores) - torch.log(torch.mean(torch.exp(marginal_scores))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_net = MineNetwork(4, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9109a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 7\n",
    "vae_hidden_dim = 64\n",
    "content_hidden_dim = 128\n",
    "num_layers = 2\n",
    "seq_length = 15\n",
    "encoded_space_dim = 4\n",
    "\n",
    "style_encoder = style_encoder(input_dim, vae_hidden_dim,\n",
    "                              num_layers, encoded_space_dim).to(device)\n",
    "decoder = Decoder(input_dim, vae_hidden_dim, num_layers, encoded_space_dim*2).to(device)\n",
    "\n",
    "VAE_model = Model(style_encoder, decoder).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(VAE_model.parameters(), lr=1e-3)\n",
    "#loss = nn.MSELoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "VAE_model.apply(initialize_weights)\n",
    "mine_net.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VAE_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9470eb",
   "metadata": {},
   "source": [
    "Discriminator base loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb07ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, seq_length, num_layers):\n",
    "        super(discriminator, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(input_size=input_dim,hidden_size=hidden_size,\n",
    "                         num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h_0)\n",
    "        out = self.fc(out[:, 10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, var, D_fake):\n",
    "    #reproduction_loss =  nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "    recon_loss = -torch.mean(torch.log(D_fake + 1e-8))\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + var - mean.pow(2) - var.exp()) / x.size(0)\n",
    "    return recon_loss, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf87ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_loc = 'scalar/'\n",
    "writer = SummaryWriter(saved_loc)\n",
    "discriminator = discriminator(input_dim=input_dim, hidden_size=hidden_size,\n",
    "                              seq_length=seq_length, num_layers=num_layers)\n",
    "\n",
    "#model.train()\n",
    "\n",
    "def train(epoch, model, VAE_model, train_loader, optimizer):\n",
    "    train_loss = 0\n",
    "    model.eval()\n",
    "    VAE_model.train()\n",
    "    mine_net.train()\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)  # [B, T, D]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z_c, _ = model(x)\n",
    "        x_hat, mu, logvar = VAE_model(x, z_c)  # model: LSTM VAE (Encoder + Decoder)\n",
    "\n",
    "        # reconstruction loss + KL divergence\n",
    "        d_real = discriminator(x)\n",
    "        d_fake = discriminator(x_hat)\n",
    "\n",
    "        recon_loss, kld = loss_function(x, x_hat, mu, logvar, d_fake)\n",
    "        mine_loss_value = mine_loss(mine_net, z_c, mu)\n",
    "        loss = recon_loss + kld + mine_loss_value * 0.1\n",
    "\n",
    "        # Logging\n",
    "        step = batch_idx + epoch * len(train_loader)\n",
    "        writer.add_scalar(\"Train/Reconstruction_loss\", recon_loss.item(), step)\n",
    "        writer.add_scalar(\"Train/KLD\", kld.item(), step)\n",
    "        writer.add_scalar(\"Train/Total_loss\", loss.item(), step)\n",
    "        writer.add_scalar(\"Train/Mine_loss\", mine_loss_value.item(), step)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #dicriminator\n",
    "        d_real = discriminator(x)\n",
    "        d_fake = discriminator(x_hat)\n",
    "\n",
    "        d_loss_real = -torch.mean(torch.log(d_real + 1e-8))\n",
    "        d_loss_fake = -torch.mean(torch.log(1 - d_fake + 1e-8))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train epoch: {epoch} [{batch_idx * len(x)}/{len(train_loader.dataset)}] '\n",
    "                  f'Loss: {loss.item()/len(x):.6f}')\n",
    "\n",
    "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
